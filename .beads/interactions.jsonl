{"id":"object-sense-03d","title":"Build CLI commands (ingest, show-object, show-type, etc.)","description":"CLI commands: ingest \u003cpath\u003e, show-object \u003cid\u003e, show-type \u003cname\u003e, show-entity \u003cid\u003e, review-types, search \u003cquery\u003e. See concept_v1.md §11 for spec.","status":"open","priority":2,"issue_type":"feature","created_at":"2025-12-22T20:45:20.354498356-08:00","updated_at":"2025-12-22T21:05:01.432039206-08:00","dependencies":[{"issue_id":"object-sense-03d","depends_on_id":"object-sense-y10","type":"blocks","created_at":"2025-12-22T20:45:59.633068279-08:00","created_by":"daemon"}]}
{"id":"object-sense-073","title":"Implement shadow-sync CLI command","description":"CLI command to sync IMI data into ObjectSense shadow graph.\n\nUsage:\n  object-sense shadow-sync /path/to/imi.db [--full|--incremental]\n\nBehavior:\n- Reads IMI sqlite using the adapter\n- Creates OS Objects, Signals, Evidence\n- Runs type inference on new observations\n- Builds/updates identity graph\n- Reports: 'Processed N photos, proposed M types, resolved K entities'\n\nThis lets you run OS on real IMI data without touching IMI's production behavior.","status":"open","priority":3,"issue_type":"task","created_at":"2025-12-24T23:29:26.420299107-08:00","updated_at":"2025-12-24T23:29:26.420299107-08:00","dependencies":[{"issue_id":"object-sense-073","depends_on_id":"object-sense-579","type":"blocks","created_at":"2025-12-24T23:30:47.172678568-08:00","created_by":"daemon"}]}
{"id":"object-sense-1i0","title":"Implement entity resolution and clustering","description":"Soft clustering of observations into entities using late fusion strategy.\n\nSignal combination (priority order):\n1. Hard IDs (product_id, SKU, trip_id) → 1.0 confidence\n2. Same-modality similarity (text↔text via BGE, image↔image via CLIP)\n3. Cross-modal similarity (image↔text via CLIP embeddings)\n\nAlgorithm: weighted_combine(signals) per design_notes.md.\n\nProto-entities compete and stabilize based on confidence scores. See concept_v1.md §4 Step 5 and §5 Layer 2.","status":"open","priority":2,"issue_type":"feature","created_at":"2025-12-22T20:45:19.536224248-08:00","updated_at":"2025-12-24T20:11:45.237156633-08:00","dependencies":[{"issue_id":"object-sense-1i0","depends_on_id":"object-sense-fj1","type":"blocks","created_at":"2025-12-22T20:45:59.518010975-08:00","created_by":"daemon"}]}
{"id":"object-sense-3rh","title":"Implement type evolution mechanisms (alias, merge, split, deprecate)","description":"Implement type evolution: alias (multiple names→one type), merge (combine types), split (create subtypes), deprecate (soft delete + migrate). See concept_v1.md §7 and design_notes.md 'Type Evolution Mechanisms'.","status":"open","priority":3,"issue_type":"feature","created_at":"2025-12-22T20:45:19.960300641-08:00","updated_at":"2025-12-22T21:05:01.229444241-08:00","dependencies":[{"issue_id":"object-sense-3rh","depends_on_id":"object-sense-y10","type":"blocks","created_at":"2025-12-22T20:45:59.7423231-08:00","created_by":"daemon"}]}
{"id":"object-sense-579","title":"Build IMI sqlite reader for ObjectSense","description":"Read-only adapter to ingest IMI's sqlite database into ObjectSense.\n\nRequirements:\n- Read from IMI's photos, metadata, raw_exif_json, burst_groups, descriptions tables\n- Convert to ObjectSense Objects, Signals, Evidence per the signal schema\n- Handle incremental sync (only process new/changed rows)\n- No writes to IMI database - strictly read-only\n\nThis is the 'parasitize' step - OS observes IMI without affecting it.","status":"open","priority":3,"issue_type":"task","created_at":"2025-12-24T23:29:26.315512995-08:00","updated_at":"2025-12-24T23:29:26.315512995-08:00","dependencies":[{"issue_id":"object-sense-579","depends_on_id":"object-sense-589","type":"blocks","created_at":"2025-12-24T23:30:47.138167985-08:00","created_by":"daemon"}]}
{"id":"object-sense-589","title":"Design signal schema for IMI→ObjectSense data flow","description":"Define how IMI's data becomes ObjectSense primitives.\n\nMapping:\n- IMI photos → OS Objects (medium=image)\n- IMI raw_exif_json → OS Object.slots + Evidence\n- IMI burst_groups → OS Signal(type='burst_group')\n- IMI quality scores → OS Signal(type='quality_sharpness', etc.)\n- IMI descriptions → OS Evidence(source=vision_model)\n- IMI embeddings → OS Signatures\n\nKey decisions:\n- Which IMI fields become slots vs evidence vs signals?\n- How to handle IMI's geocoding_cache → OS place entities?\n- Schema for temporal proximity grouping (generic burst detection)\n\nOutput: Design doc or code comments defining the contract.","status":"open","priority":3,"issue_type":"task","created_at":"2025-12-24T23:28:59.413399395-08:00","updated_at":"2025-12-24T23:28:59.413399395-08:00","dependencies":[{"issue_id":"object-sense-589","depends_on_id":"object-sense-fj1","type":"blocks","created_at":"2025-12-24T23:30:47.293345275-08:00","created_by":"daemon"}]}
{"id":"object-sense-5hx","title":"Build Sparkstation embedding client","description":"Create async client wrapper for Sparkstation embedding endpoints.\n\nCapabilities needed:\n- embed_text(texts: list[str]) → list[list[float]] using bge-large (1024-dim)\n- embed_image(images: list[bytes|str]) → list[list[float]] using clip-vit (768-dim)\n- embed_text_clip(texts: list[str]) → list[list[float]] using clip-vit (768-dim)\n\nUse OpenAI SDK with base_url=settings.llm_base_url.\nHandle batching, retries, and errors gracefully.\n\nBlocked by: nothing (can start now)\nBlocks: object-sense-mnz (feature extraction needs this)","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-24T20:12:07.402922435-08:00","updated_at":"2025-12-24T20:32:56.770155793-08:00","closed_at":"2025-12-24T20:32:56.770155793-08:00","close_reason":"Implemented EmbeddingClient with all 3 methods. Unit tests (13) + integration tests (2) pass. Manual verification against live Sparkstation confirmed."}
{"id":"object-sense-a1a","title":"Implement medium probing and affordance detection","description":"Detect medium (image/text/json/video/audio/binary) from file headers, extensions, entropy. Map mediums to affordances (can_embed, can_detect_objects, etc.). See concept_v1.md §3.2.","status":"closed","priority":2,"issue_type":"feature","created_at":"2025-12-22T20:45:18.912321271-08:00","updated_at":"2025-12-24T20:52:14.062610896-08:00","closed_at":"2025-12-24T20:52:14.062610896-08:00","close_reason":"Closed","dependencies":[{"issue_id":"object-sense-a1a","depends_on_id":"object-sense-y10","type":"blocks","created_at":"2025-12-22T20:45:59.13866307-08:00","created_by":"daemon"}]}
{"id":"object-sense-cqz","title":"OPEN: Unified feature space alignment (visual/textual/structural)","description":"Open question: How do visual embeddings (CLIP), text embeddings, and structural features (schema hashes) align in a unified space? Needed for cross-medium entity clustering. See concept_v1.md §12.","status":"closed","priority":3,"issue_type":"task","created_at":"2025-12-22T20:45:30.801514541-08:00","updated_at":"2025-12-24T20:04:42.662705291-08:00","closed_at":"2025-12-24T20:04:42.662705291-08:00","close_reason":"Resolved: Late fusion with separate embedding columns (text_embedding 1024, image_embedding 768, clip_text_embedding 768). See design_notes.md."}
{"id":"object-sense-czu","title":"Set up project structure (Python/FastAPI)","description":"Initialize pyproject.toml with UV, create src/object_sense/ layout, configure ruff + pyright, add basic FastAPI app skeleton. See concept_v1.md §11 for tech stack.","status":"closed","priority":1,"issue_type":"task","created_at":"2025-12-22T20:45:18.628102441-08:00","updated_at":"2025-12-22T21:15:36.941493459-08:00","closed_at":"2025-12-22T21:15:36.941493459-08:00","close_reason":"Closed"}
{"id":"object-sense-ds4","title":"Fix LiteLLM tool calling passthrough in Sparkstation","description":"Tool calls are returned as content instead of tool_calls field.\n\nRoot cause: gateway/litellm.yaml has drop_params: true which drops the 'tools' parameter before it reaches vLLM.\n\nFix options:\n1. Set drop_params: false\n2. Add supports_function_calling: true per model\n3. Use allowed_params to whitelist tools/tool_choice\n\nFiles to modify:\n- gateway/litellm.yaml\n\nThis blocks ObjectSense type inference from using RAG tools.","status":"open","priority":2,"issue_type":"bug","created_at":"2025-12-25T00:30:52.651842231-08:00","updated_at":"2025-12-25T00:30:52.651842231-08:00"}
{"id":"object-sense-egs","title":"Implement temporal proximity grouping (generic burst detection)","description":"ObjectSense-level primitive for 'same scene, multiple observations'.\n\nThis is the physics, not the semantics:\n- Temporal proximity clustering (observations close in time)\n- Near-identical embedding similarity\n- Output: Signal(type='temporal_group', group_id=..., confidence=...)\n\nUse cases across domains:\n- Wildlife: burst of photos in 0.3s\n- Video: adjacent frames of same scene\n- Security: movement sequence\n- Inventory: multi-angle product shots\n\nIMI adds workflow semantics on top (representative selection, culling UX).\nObjectSense owns the grouping primitive.\n\nImplementation:\n- Configurable time threshold (default: 0.5s)\n- Optional embedding similarity check\n- Camera/source bucketing","status":"open","priority":3,"issue_type":"feature","created_at":"2025-12-24T23:30:58.669298486-08:00","updated_at":"2025-12-24T23:30:58.669298486-08:00"}
{"id":"object-sense-fj1","title":"Implement LLM integration for type inference (tool calling)","description":"Integrate local LLM via tool calling for type inference. LLM queries type store (RAG), proposes primary_type + slots + maybe_new_type. 100% structured outputs. See concept_v1.md §4 Step 4 and §8.2.","status":"closed","priority":2,"issue_type":"feature","created_at":"2025-12-22T20:45:19.156750882-08:00","updated_at":"2025-12-25T00:03:27.259610388-08:00","closed_at":"2025-12-25T00:03:27.259610388-08:00","close_reason":"Closed","dependencies":[{"issue_id":"object-sense-fj1","depends_on_id":"object-sense-mnz","type":"blocks","created_at":"2025-12-22T20:45:59.401635524-08:00","created_by":"daemon"}]}
{"id":"object-sense-g9f","title":"Shadow-mode convergence with image_metadata_indexing","description":"Epic: Run ObjectSense in shadow mode on IMI data to prove the substrate before any migration.\n\nStrategy: 'Signals in, substrate out'\n- IMI stays production-pure, unchanged\n- ObjectSense reads IMI's sqlite (read-only)\n- OS builds identity graph, proposes types, tracks provenance\n- IMI does NOT consume OS output until Phase 2\n\nPhases:\n1. Shadow mode (OS processes IMI data, no consumption)\n2. First consumption (blob dedup OR place entities)\n3. Earned dependency (re-ID, cross-session entities)\n\nSuccess metric: 'If ObjectSense disappeared tomorrow, would IMI notice?'\n- Short term: No → good\n- Mid term: Slightly → ideal  \n- Long term: Yes → success","status":"open","priority":3,"issue_type":"feature","created_at":"2025-12-24T23:28:51.072974658-08:00","updated_at":"2025-12-24T23:28:51.072974658-08:00"}
{"id":"object-sense-i3t","title":"Phase 2: First consumption - blob dedup or place entities","description":"First point where IMI actually consumes ObjectSense output.\n\nOptions (pick one):\n1. Blob dedup: IMI queries OS for 'have I seen these bytes before?'\n2. Place entities: IMI uses OS's resolved location entities instead of raw GPS\n\nSuccess criteria:\n- IMI code calls OS (in-process, library mode)\n- One concrete feature improves\n- Minimal coupling (can still run without OS)\n\nThis is the 'slightly notice if OS disappeared' milestone.","status":"open","priority":4,"issue_type":"task","created_at":"2025-12-24T23:29:27.323331549-08:00","updated_at":"2025-12-24T23:29:27.323331549-08:00","dependencies":[{"issue_id":"object-sense-i3t","depends_on_id":"object-sense-rck","type":"blocks","created_at":"2025-12-24T23:30:47.233388083-08:00","created_by":"daemon"}]}
{"id":"object-sense-jmt","title":"Verify TypeInferenceAgent works with Sparkstation tool calling","description":"After fixing Sparkstation tool calling (object-sense-ds4), verify that:\n\n1. TypeInferenceAgent can call search_types, get_type_details, find_similar_objects tools\n2. The full pydantic-ai agent works end-to-end with gpt-oss-20b\n3. Integration tests pass\n\nRun: pytest tests/test_type_inference.py -m integration","status":"open","priority":2,"issue_type":"task","created_at":"2025-12-25T00:30:59.684375795-08:00","updated_at":"2025-12-25T00:30:59.684375795-08:00","dependencies":[{"issue_id":"object-sense-jmt","depends_on_id":"object-sense-ds4","type":"blocks","created_at":"2025-12-25T00:31:03.622348914-08:00","created_by":"daemon"}]}
{"id":"object-sense-mnz","title":"Implement feature extraction per medium (image, text, JSON)","description":"Build extractors per medium using Sparkstation models.\n\nEmbeddings per medium (late fusion strategy):\n- Image: image_embedding (CLIP visual 768) + text_embedding (caption→BGE 1024)\n- Text: text_embedding (BGE 1024) + clip_text_embedding (CLIP text 768)\n- JSON: text_embedding (fields→BGE 1024) + hash_value (schema hash)\n- Video: image_embedding (keyframe pool) + text_embedding (transcript→BGE)\n\nModels: bge-large (text), clip-vit (image/text cross-modal).\nOutput: Signature records with embeddings + hash_value + extra metadata.\n\nSee concept_v1.md §4 Step 3, §6, and design_notes.md (late fusion section).","status":"closed","priority":2,"issue_type":"feature","created_at":"2025-12-22T20:45:19.030456528-08:00","updated_at":"2025-12-24T23:19:21.331023357-08:00","closed_at":"2025-12-24T23:19:21.331023357-08:00","close_reason":"Implemented feature extraction for image/text/JSON with late-fusion embeddings (CLIP visual, BGE text, CLIP text cross-modal). 33 tests, all passing.","dependencies":[{"issue_id":"object-sense-mnz","depends_on_id":"object-sense-a1a","type":"blocks","created_at":"2025-12-22T20:45:59.285341832-08:00","created_by":"daemon"},{"issue_id":"object-sense-mnz","depends_on_id":"object-sense-5hx","type":"blocks","created_at":"2025-12-24T20:12:11.693932942-08:00","created_by":"daemon"}]}
{"id":"object-sense-rck","title":"Build shadow dashboard for OS vs IMI comparison","description":"Visualization to compare ObjectSense's view vs IMI's current state.\n\nShows:\n- Types proposed by OS (species taxonomy, location hierarchy)\n- Entity clusters OS found (same individual across sessions?)\n- Dedup candidates OS identified that IMI missed\n- Provenance chains (why does OS believe X?)\n\nPurpose: Prove OS is adding value before any integration.\nThis is how we know Phase 2 is ready.\n\nCould be CLI tables, simple web UI, or notebook.","status":"open","priority":4,"issue_type":"task","created_at":"2025-12-24T23:29:26.529995438-08:00","updated_at":"2025-12-24T23:29:26.529995438-08:00","dependencies":[{"issue_id":"object-sense-rck","depends_on_id":"object-sense-073","type":"blocks","created_at":"2025-12-24T23:30:47.202932581-08:00","created_by":"daemon"}]}
{"id":"object-sense-wwo","title":"Phase 3: Cross-session entity re-identification","description":"The big payoff: 'Show me the same leopard across 3 trips.'\n\nRequirements:\n- Visual similarity model for individual re-ID\n- Temporal/spatial context signals\n- Confidence scoring and conflict resolution\n- Human correction loop (user confirms/denies matches)\n- Evidence accumulation over time\n\nThis is 6+ months out. Tracking to remember the goal.\n\nSuccess: IMI can answer queries that require persistent identity across sessions, powered by ObjectSense's entity graph.","status":"open","priority":4,"issue_type":"task","created_at":"2025-12-24T23:29:28.891704546-08:00","updated_at":"2025-12-24T23:29:28.891704546-08:00","dependencies":[{"issue_id":"object-sense-wwo","depends_on_id":"object-sense-i3t","type":"blocks","created_at":"2025-12-24T23:30:47.263052814-08:00","created_by":"daemon"}]}
{"id":"object-sense-x11","title":"OPEN: Proto-entity decay policy","description":"Open question: Proto-entities that never stabilize should be pruned, but rare-but-real entities shouldn't. Need decay based on confidence, not just time/existence. See concept_v1.md §12.","status":"open","priority":4,"issue_type":"task","created_at":"2025-12-22T20:45:31.112999967-08:00","updated_at":"2025-12-22T21:05:02.240875265-08:00"}
{"id":"object-sense-x98","title":"OPEN: Evidence compaction strategy","description":"Open question: Storing every Evidence record forever doesn't scale. Need strategy to compact/summarize evidence once confidence stabilizes. See concept_v1.md §12.","status":"open","priority":4,"issue_type":"task","created_at":"2025-12-22T20:45:31.003439848-08:00","updated_at":"2025-12-22T21:05:02.024271298-08:00"}
{"id":"object-sense-y10","title":"Design and implement core data model (Objects, Types, Entities, Evidence)","description":"Implement Postgres schema for core primitives: Object, Type, Entity, Evidence, Signatures. See concept_v1.md §3 and §9 for data model. Use SQLAlchemy or raw SQL with asyncpg.","status":"closed","priority":1,"issue_type":"feature","created_at":"2025-12-22T20:45:18.785546533-08:00","updated_at":"2025-12-22T21:27:09.158812896-08:00","closed_at":"2025-12-22T21:27:09.158812896-08:00","close_reason":"Closed","dependencies":[{"issue_id":"object-sense-y10","depends_on_id":"object-sense-czu","type":"blocks","created_at":"2025-12-22T20:45:56.423723584-08:00","created_by":"daemon"}]}
{"id":"object-sense-yna","title":"OPEN: Fast-path vs slow-path latency strategy","description":"Open question: LLM inference on every blob is expensive. Need fast-path (hash match, deterministic IDs) that skips LLM vs slow-path (full semantic analysis). See concept_v1.md §12.","status":"open","priority":3,"issue_type":"task","created_at":"2025-12-22T20:45:30.90684612-08:00","updated_at":"2025-12-22T21:05:01.824452012-08:00"}
